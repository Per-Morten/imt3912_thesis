\subsection{Lock-Free Allocator}
\label{subsec:detailed_lock_free_allocator}
The lock free allocator is a linear allocator built with a linked list of memory blocks.
It was introduced into the NOX ECS to support dynamic allocation mainly for the event system,
however it was soon used for other systems as well, like the queuing mechanisms in the entity manager.
\todo{Mention that this is for short lived memory}

\subsubsection{Explanation}
As mentioned earlier the allocator is a linear allocator,
this means that while the allocator supports allocations of many different sizes,
however it only supports mass deallocation, i.e. freeing all memory at once.
This decision was intentionally made to keep the functionality of the allocator as simple
as possible, to allow for the lock free implementation.
Another rationale for the decision to go for a linear allocator was that the allocator
is usually used for objects who has a short lifetime, like events or transition requests.
These objects only live for a single iteration of the update loop, and are all destroyed at
the same location, meaning that the support for individual deallocation is not really needed.

\paragraph{Compare and swap}
\todo{Does this need to be explained}

\paragraph{Allocate}
\lstinputlisting[language=cpp, caption=allocate, label=lst:allocate]{lock_free_allocate.py}

\paragraph{Try Allocate}
\lstinputlisting[language=cpp, caption=allocate, label=lst:try_allocate]{lock_free_try_allocate.py}

\subsubsection{Motivation}
There were many factors influencing the decision to create this allocator structure.

\paragraph{Context Switch}
One of the issues with the standard heap allocation operations in C++ is that
on a lot of operating systems the request for memory allocations leads to a context switch into
kernel mode and back to user mode, this is also the case for deallocation.
The context switch can be a very costly operation, and should be avoided
if possible, especially in performance intensive parts of the program\cite[p. 240]{game_engine_architecture}.
A way to avoid this context switch is to reuse memory, which can be done by allocating
chunks of memory up front, and allocating into that instead.

\paragraph{Avoiding Locking}
The allocator was supposed to be used in a multi-threaded environment, where multiple threads
should be able to allocate from the same allocator.
Having shared mutable state between threads means that some sort of protection is needed
to avoid race condition problems.
One solution would be a mutual exclusion strategy, locking access to the allocator state through a mutex.
However, this approach can be quite costly, especially in systems where there will be high contention,
like the event system.
It was therefore decided that it would be better to go with an atomic lock-free approach to the allocator.

\paragraph{Performance on Weakly Ordered Systems}
The other rationale for the lock-free structure is to increase performance on weakly ordered systems,
like ARM architectures. \todo{Add ref, Preshing for example}
Working with low level atomics in C++ allows for the specification of memory ordering constraints,
which can be used to improve performance on the weakly ordered systems.
\todo{Add some section explaining the lock free stuff properly?, or just do it in the explanation section?}

\paragraph{Pointer Invalidation}
Seeing as the allocator would be accessed from different threads concurrently it was important that
memory would not be reallocated, as this would destroy references to objects within the allocated memory.
This is what motivated the linked list approach of the allocator, which allows for insertion of elements without
the need for reallocation.

\paragraph{Read Only Values}
\todo{Rewrite/Rethink this paragraph}
The allocator would mostly be used for read only like operations, like events within the event system.
These events are usually just created, and sent through the event system, they are usually not modified after creation.

\subsubsection{Alternatives}
Probably best alternative solution would be to just give each thread their own allocator.
This solution would remove the synchronization and protection requirements, as no data would need to be shared between threads.
%Having one allocator per thread could also solve potential cache ping-ponging and false sharing issues.
In hindsight this would possibly have been a better solution than the one chosen, it would at least have been easier to implement.
\todo{Elaborate}

\subsubsection{Pros}
No chances of deadlock,
Wait free, (or just lock-free need to check)
Reuses memory,
should not need to context switch, except when allocating a new block.

\subsubsection{Cons}
Code is hard to debug,
May be allocating more than needed,
Much simultaneous access to an allocator that needs to allocate a block will
allocate and free a lot of extra blocks,
Currently does not support shrinking functionality,
Blocks are linked list based, meaning that some caching is lost,
however it is needed to not invalidate pointers.
Memory is fragmented, (Should not be such a big issue seeing as it is meant for single frame stuff)
Meta-Data is stored together with the data, optimal for allocation and deallocation,
not optimal for accessing the memory.
Cache lines are possibly sent much back and forth between the different threads.
