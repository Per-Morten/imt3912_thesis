\section{Identifying Metrics}

\subsection{Main metrics}
Analyzing the difference between the NOX Actor system and the NOX ECS based on the requirements would need different metrics.
After some discussion we decided to use time, memory usage and cpu cycles.
Not all of these were equally important, for instance, the cpu cycles would mainly be used if there were small differences on the time.

\subsection{Time}
The time was measured with the use of the \emph{time}\footnote{\url{https://linux.die.net/man/1/time}} function in the Linux command line.
Time was chosen as a metric because how it relates to the real-time requirements of games\secref{subsec:introduction_game_industry_performance}.
Additionally, it is a more relatable and overall easier metric than for example cycles.
The time command outputs three different measurements, which are the following\cite{stack_overflow_time_output}.

\begin{itemize}
    \item Real time
    \item User time
    \item Sys time
\end{itemize}

\paragraph{Real Time}
Real time measures the wall clock time, which details how long a test spent running from the start to the end of the call.
There can be problems using this measurement, as one has little to no control on when the operative system context switches, and how different processes are scheduled.
This problem can be mitigated to a large extent by running the tests with no other programs running at the same time, giving the tests full access to all the CPU's.

\paragraph{User Time}
User time measures the total time spent in user-mode of the program being timed.
This means the total time of all CPU's have spent outside of the kernel.
In contrast to the real measurement, user measurement is not affected by the operative system.
This is because the other processes and time the processes spends blocked does not count.

\paragraph{Sys Time}
Sys time is very similar to the User measurement, except that it counts the time spent in kernel-mode, rather than user-mode.
Adding the user and sys measurements will tells us exactly how much CPU time was spent on a process.
Sys time is the total time across all CPU's, meaning that if the tests are run on a system with multiple threads, the sum of user and sys time could exceed the real time measurement.
Using the combined time of user and sys would work if all tests run in a single threaded environment.

\paragraph{Decision}
We will use real time as our time metric as it is the only viable option when running in a concurrent environment.
To mitigate the problem of context switches the tests will be run in isolation, decreasing the frequency of context switches and other processes interfering.

\subsection{Cycles}
The cycle metric quantifies how many CPU ticks took place from start to finish for each test.
Collecting data involves running assembly code with an instruction called Read Time-Stamp Counter, or rdtsc for short\cite{stack_overflow_rdtcs}.
The instruction is used to determine how many CPU ticks have occurred since the processor was reset.
Note that because the size of the number is limited to the register it is stored in, it will at some point overflow and reset back to 0.
The time it takes for a CPU to overflow in this manner, will depend on if the CPU has 32 or 64 bit registers.
How long each amount of bits will take before overflowing varies drastically between the two.
The duration can be calculated with formula \ref{eq:rdtsc_overflow_duration}, where x is the duration in seconds, h is the tick rate of the CPU, and b is the bit width of the register.

\begin{equation}\label{eq:rdtsc_overflow_duration}
x_b = \frac{2^b - 1}{h}
\end{equation}

\bigskip


\noindent
Assuming a \SI{2}{\giga\hertz} tick rate we can calculate the time it would take before an overflow would occur.
Having a 32 bit register for measuring tick rate is not sufficient, as it would overflow in only two seconds, as seen in formula \ref{eq:rdtsc_overflow_duration_32}.
This makes it likely to overflow in a lot of tests, even if they don't last a full 2 seconds.
In contrast, a 64 bit register has a much longer duration of approximately 292 years, as seen in formula \ref{eq:rdtsc_overflow_duration_64},
which makes it highly unlikely that any tests are affected by a potential overflow.
Additionally, even if a test was affected by overflow, it would be easily noticeable on our graphs,
as the difference between pre and post overflow would be clearly identifiable.
If this occurred, the single test affected could easily be run a second time.

\begin{equation}\label{eq:rdtsc_overflow_duration_32}
x_{32} = \frac{2^{32} - 1}{2 \times 10^{9}} = \SI{2.5}{\second}
\end{equation}

\begin{equation}\label{eq:rdtsc_overflow_duration_64}
x_{64} = \frac{2^{64} - 1}{2 \times 10^{9}} = \SI{9223372036.85}{\second} \approx \SI{292}{\year}
\end{equation}

\bigskip

The test were run on a two year old laptop (see table \ref{tab:hardware_info}) with a 64 bit operating system. Additionally tests output correct numbers outside the 32 bit range,
clearly indicating we are running with a 64 bit register.

Counting cycles will give a very consistent metric in addition to measuring real time.
This will strengthen the validity of the time metrics, by having another measurement to further confirm the results.

\subsection{Memory Usage}
\label{subsec:measurements_massif}
The memory usage is measured with the heap profiler Massif, which is a part of the tool suite called Valgrind.
Massif\cite{massif_manual} takes snapshots of the processes heap each time a allocation or deallocation happens, but reduces this frequency if the process runs for a prolonged duration.
The snapshots can come in two different forms, normal and detailed.
Normal only displays basic information with the time, total bytes consumed, and a few other data points.
Detailed additionally collects the stack trace of every single allocation point in the program.
This data is visualized through a three, and gives a full picture of how and why all the heap memory was allocated.
There is also a peak snapshot, at which point the heap consumption was at its greatest.

Below is an example of the part of a massif output, where row 10 to 13 are normal snapshots, and the row 14 is a detailed one.

\begin{lstlisting}[basicstyle=\footnotesize, caption={[Massif output example]Massif output example (massif documentation\protect\cite{massif_manual})}, label=lst:massif_output]
--------------------------------------------------------------------------------
 n        time(B)         total(B)   useful-heap(B) extra-heap(B)    stacks(B)
--------------------------------------------------------------------------------
10         10,080           10,080           10,000            80            0
11         12,088           12,088           12,000            88            0
12         16,096           16,096           16,000            96            0
13         20,104           20,104           20,000           104            0
14         20,104           20,104           20,000           104            0
99.48% (20,000B) (heap allocation functions) malloc/new/new[], --alloc-fns, etc.
->49.74% (10,000B) 0x804841A: main (example.c:20)
|
->39.79% (8,000B) 0x80483C2: g (example.c:5)
| ->19.90% (4,000B) 0x80483E2: f (example.c:11)
| | ->19.90% (4,000B) 0x8048431: main (example.c:23)
| |
| ->19.90% (4,000B) 0x8048436: main (example.c:25)
|
->09.95% (2,000B) 0x80483DA: f (example.c:10)
->09.95% (2,000B) 0x8048431: main (example.c:23)
\end{lstlisting}

\subsection{Additional Data}
\label{subsec:measurements_callgrind}
Additional data points are gathered from Callgrind\cite{callgrind_manual}, which is another profiling tool in the Valgrind tool suite.
Callgrind collects various data from an executed program, and visualizes the information as a call graph.
This data are not used as extensively as the three mentioned above,
but are for tests that need further inspection and requires greater detail.
The data collected involves, but is not limited to.

\begin{itemize}
    \item Instructions executed
    \item Memory reads
    \item Memory writes
    \item first and last level instruction cache read misses
    \item first and last level data cache read misses
    \item first and last level data cache write misses
    \item Conditional branches executed
    \item Conditional branches mispredicted
    \item Indirect branches executed
    \item Indirect branches mispredicted
\end{itemize}

